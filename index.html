<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhen Yang (杨珍)</title>

  <meta name="author" content="Zhen Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ut_icon.png">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhen Yang  &nbsp&nbsp   杨珍 </name>
              </p>
              <p>
                I am a PhD student at <a href="https://keg.cs.tsinghua.edu.cn/">Knowledge Engineering Group (KEG)</a>, Department of Computer Science and Technology of Tsinghua University, fortunately working with <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Prof. Jie Tang</a>. I obtained my master and bachelor degree from <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a> and <a href="https://en.xidian.edu.cn/">Xidian University</a>, respectively.
              </p>
              <p>
                My current research interests lie in specialized mathematical models, large language models, multi-modal large language models, and graph representation learning.  If these areas align with your interests, I welcome you to reach out via email. I am always open to discussing potential collaborations and exploring new ideas together.
              </p>
              <p style="text-align:center">
                <a href="mailto:yangz21@mails.tsinghua.edu.cn">Email: yangz21 [at] mails.tsinghua.edu.cn</a> &nbsp/&nbsp
                <a href="data/yz_academic.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=zPVItDgAAAAJ&hl=zh-en">Google Scholar</a> &nbsp/&nbsp
		<a href="https://github.com/zyang-16/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yz.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/yz.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <div style="width:100%;overflow-y:scroll; height:230px;">
                <ul>
                  <li style="line-height:30px"> <b>August 2024:</b> Checkout our specialized mathematical multi-modal large language model <b>MathGLM-Vision</b>!</li>
                  <li style="line-height:30px"> <b>August 2024:</b> Checkout our multi-modal scientific benchmark <b>VisScience</b>!</li>
		  <li style="line-height:30px"> <b>June 2024:</b> Checkout our GLM Team paper <a href="https://arxiv.org/abs/2406.12793">ChatGLM</a>!</li>
                  <li style="line-height:30px"> <b>February 2024:</b> Our <a href="https://ieeexplore.ieee.org/abstract/document/10454000">survey</a> paper about <b>Negative Sampling</b> is accepted by <b>TPAMI 2024</b>!</li>
                  <li style="line-height:30px"> <b>December 2023:</b> Our paper <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779TriSampler">TriSampler</a> is accepted by <b>AAAI 2024</b>!</li>
                  <li style="line-height:30px"> <b>September 2023:</b> Checkout our specialized mathematical model <a href="https://arxiv.org/abs/2309.03241"><b>MathGLM</b></a>!</li>
		  <li style="line-height:30px"> <b>September 2023:</b> I am currently doing an internship at the <a href="https://chatglm.cn/main/alltoolsdetail">ChatGLM</a> group of <a href="https://www.zhipuai.cn/">ZhipuAI</a>!</li>
		  <li style="line-height:30px"> <b>August 2023:</b> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.html">ViLTA</a> gets accepted to <b>ICCV 2023</b>!</li>
                  <li style="line-height:30px"> <b>May 2023:</b> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">BatchSampler</a> gets accepted to <b>KDD 2023</b>!</li>
                  <li style="line-height:30px"> <b>February 2022:</b> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/9723516">RecNS</a> gets accepted to <b>TKDE 2022</b>!</li>
                  <li style="line-height:30px"> <b>January 2022:</b> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512041">STAM</a> gets accepted to <b>WWW 2022</b>!</li>
		  <li style="line-height:30px"> <b>May 2021:</b> One paper <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">MixGCF</a> gets accepted to <b>KDD 2021</b>!</li>
                  <li style="line-height:30px"> <b>May 2020:</b> One paper <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">MCNS</a> gets accepted to <b>KDD 2020</b>!</li>
                <ul>
                </div>
		    </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mathglm-vision.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2012.13587">
                <papertitle>MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model</papertitle>
              </a>
              <br>
	      Zhihuan Jiang*,
              <strong>Zhen Yang*</strong>,
              Jinhao Chen,
              Zhengxiao Du,
              Weihan Wang,
              Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>Manuscript</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2012.13587">paper</a>,
              <a href="https://github.com/THUDM/VisScience">code</a>
              <p></p>
              <p>We proposed a new mutant of dilated convolution, namely inception (dilated) convolution where the convolutions have independent dilation among different axes, channels and layers.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/visscience.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.pdf">
                <papertitle>VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Jinhao Chen*,
              Zhengxiao Du,
              Wenmeng Yu,
	      Weihan Wang,
	      Wenyi Hong,
	      Zhihuan Jiang,
	      Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>Manuscript</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.pdf">paper</a>,
              <a href="https://github.com/THUDM/MathGLM-Vision">code</a>
              <p></p>
              <p>We apply our techniques to produce fully quantized 4-bit detectors based on RetinaNet and Faster RCNN, and show that these achieve state-of-the-art performance for quantized detectors.</p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/survey.png" alt="streamv2v" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10454000">
                <papertitle>Does Negative Sampling Matter? A Review with Insights into its Theory and Applications</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
              Tinglin Huang,
              Yukuo Cen,
              Junshuai Song,
              Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>TPAMI</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10454000">paper</a>
              <p></p>
              <p>We present StreamV2V to support real-time video-to-video translation for streaming input.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/trisampler.png" alt="ovseg" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779">
                <papertitle>TriSampler: A Better Negative Sampling Principle for Dense Retrieval</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Zhou Shao,
              Yuxiao Dong,
              Jie Tang,
              <br>
              <em>AAAI</em>, 2024
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779">paper</a>
              <p></p>
              <p>We design the quasi-triangular principle and introduce TriSampler to selectively sample more informative negatives within a prescribed constrained region.</p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mathglm.png" alt="ovseg" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.03241">
                <papertitle>GPT Can Solve Mathematical Problems Without a Calculator</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Ming Ding*,
              Qingsong Lv,
              Zhihuan Jiang,
              Zehai He,
              Yuyi Guo,
              Jinfei Bai,
              Jie Tang,
              <br>
              <em>Manuscript</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2309.03241">arxiv</a>,
              <a href="https://github.com/THUDM/MathGLM">code</a>
              <p></p>
              <p>We propose a 2 billion-parameter language model MathGLM that can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage.</p>
            </td>
          </tr>
	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/batchsampler.png" alt="supmae" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">
                <papertitle>Batchsampler: Sampling Mini-batches for Contrastive Learning in Vision, Language, and Graphs</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Tinglin Huang*,
	      Ming Ding*,
	      Yuxiao Dong,
	      Rex Ying,
	      Yukuo Cen,
	      Yangliao Geng,
	      Jie Tang
              <br>
              <em>KDD</em>, 2023
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">paper</a>,
              <a href="https://github.com/THUDM/BatchSampler">code</a>
              <p></p>
              <p>We present BatchSampler to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances by leveraging the proximity graph and a random walk with restart.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vilta.png" alt="declip" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.pdf">
                <papertitle>ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation</papertitle>
              </a>
              <br>
              Weihan Wang*,
              <strong>Zhen Yang*</strong>,
              Bin Xu,
              Juanzi Li,
              Yankui Sun
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.pdf">paper</a>
              <p></p>
              <p>We propose a novel method ViLTA that utilize a cross-distillation method to generate soft labels for enhancing the robustness of model.</p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stam.png" alt="ant" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>STAM: A Spatiotemporal Aggregation Method for Graph Neural Network-based Recommendation</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
              Bin Xu,
              Hongxia Yang,
	      Jie Tang
              <br>
              <em>WWW</em>, 2022
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512041">paper</a>,
              <a href="https://github.com/zyang-16/STAM">code</a>
              <p></p>
              <p>We propose a spatiotemporal aggregation method STAM to efficiently incorporate temporal information into neighbor embedding learning.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/recns.png" alt="repre" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9723516">
                <papertitle>Region or Global? A Principle for Negative Sampling in Graph-based Recommendation</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
	      Xu Zou,
	      Jie Tang,
	      Bin Xu,
	      Chang Zhou,
	      Hongxia Yang
              <br>
              <em>TKDE</em>, 2022, <b>Long oral</b>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9723516">paper</a>,
              <a href="https://github.com/zyang-16/RecNS">code</a>
              <p></p>
              <p>We design three region principle to select negative candidate and propose RecNS method to sythesize hard negatives.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mixgcf.png" alt="crnas" width="280" height="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">
                <papertitle>MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</papertitle>
              </a>
              <br>
	      Tinglin Huang,
	      Yuxiao Dong,
	      Ming Ding,
              <strong>Zhen Yang</strong>,
	      Wenzheng Feng,
	      Xinyu Wang,
	      Jie Tang
              <br>
              <em>KDD</em>, 2021
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">paper</a>,
              <a href="https://github.com/huangtinglin/MixGCF">code</a>
              <p></p>
              <p>We present MixGCF that can study negative sampling by leveraging both the user-item graph structure and GNNs’ aggregation process to design the hop mixing technique to synthesize hard negatives.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mcns.png" alt="oqa" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">
                <papertitle>Understanding Negative Sampling in Graph Representation Learning</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Ming Ding*,
              Chang Zhou,
              Hongxia Yang,
              Jingren Zhou,
              Jie Tang,
              <br>
              <em>KDD</em>, 2020
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">paper</a>,
              <a href="https://github.com/THUDM/MCNS">code</a>
              <p></p>
              <p>We develop a theory and quantify that a nice negative sampling distribution is \( p_n(u|v) \propto p_d(u|v)^\alpha \), \( 0 < \alpha < 1 \). Additionly, we propose Markov chain Monte Carlo Negative Sampling (MCNS), an effective and scalable negative sampling strategy for various tasks in graph representation learning.</p>
            </td>
          </tr>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
                <p>
                  <li style="line-height:30px"> Reviewer of Journals: TKDE, TPAMI</li>
                  <li style="line-height:30px"> Reviewer of Conferences: KDD 2021/2022, WWW 2023/2024/2025</li>
                </p>
            </td>
          </tr>
        </tbody></table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
                <ul>
                  <li style="line-height:30px"> Huawei Scholarship, 2023.</li>
                  <li style="line-height:30px"> Excellent Graduate of Beijing, 2019.</li>
                  <li style="line-height:30px"> Outstanding Academic Paper Award, Tsinghua University, 2019.</li>
		  <li style="line-height:30px"> 129 Scholarship of Tsinghua University, 2018.</li>	
		  <li style="line-height:30px"> National Scholarship by Ministry of Education of China, 2018.</li>
                  <li style="line-height:30px"> National Scholarship by Ministry of Education of China, 2014.</li>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <a href="https://clustrmaps.com/site/1bhpp" title="Visit tracker">
                <img src="//www.clustrmaps.com/map_v2.png?d=6oa3ivKJIw5Vmqg_fFtgZxTmVsyrTJMJ_XKxZlDEsRI&cl=ffffff">
              </a>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
