<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhen Yang (杨珍)</title>

  <meta name="author" content="Zhen Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ut_icon.png">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhen Yang  &nbsp&nbsp   杨珍 </name>
              </p>
              <p>
                I am a postdoctoral researcher in the Department of Computer Science and Technology, Tsinghua Univerisity, collaborated with  <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Prof. Jie Tang</a>. Prior to that, I received my Ph.D. from the <a href="https://keg.cs.tsinghua.edu.cn/">Knowledge Engineering Group (KEG)</a>, Department of Computer Science and Technology of Tsinghua University, fortunately working with <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Prof. Jie Tang</a>. I obtained my master and bachelor degree from <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a> and <a href="https://en.xidian.edu.cn/">Xidian University</a>, respectively.
              </p>
              <p>
                My research interests lie in <b>vision-language models</b>, <b>large language models</b>, and <b>graph representation learning</b>. Currently, I focus on multimodal coding agents and agentic reinforcement learning. I aim to build agentic systems that can reason, plan, and generate executable code cross modalities. 
              </p>
			  <p>
				If these areas align with your interests, I welcome you to reach out via email. I am always open to discussing potential collaborations and exploring new ideas together.
			  </p>
              <p style="text-align:center">
                <a href="mailto:yang-zhen@mail.tsinghua.edu.cn">Email: yang-zhen [at] mail.tsinghua.edu.cn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=zPVItDgAAAAJ&hl=zh-en">Google Scholar</a> &nbsp/&nbsp
		<a href="https://github.com/zyang-16/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yz.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/yz.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <div style="width:100%;overflow-y:scroll; height:230px;">
                <ul>
				  <li style="line-height:30px"><b>January 2026:</b> Our <a href="https://arxiv.org/pdf/2510.18798">WebSeer</a> paper about <b>Deeper Search Agents</b> is accepted by <b>ICLR 2026</b>!</li>
				  <li style="line-height:30px"><b>December 2025:</b> Released <b>GLM-4.6V</b>, our newest open-source GLM-4.6V series model, including GLM-4.6V (106B-A12B) and GLM-4.6V-Flash (9B).</li>
				  <li style="line-height:30px"><b>November 2025:</b> Our <a href="https://arxiv.org/pdf/2511.06805">MathSE</a> paper about <b>Multimodal Mathematical Reasoning</b> is accepted by <b>AAAI 2026</b>!</li>
				  <li style="line-height:30px"><b>August 2025:</b> Released <b>GLM-4.5V</b>, an improved vision-language model across multiple benchmarks.</li>
				 <li style="line-height:30px"><b>July 2025:</b> Open-sourced <b>GLM-4.1V-Thinking</b>, designed to advance general-purpose multimodal understanding
and reasoning.</li>
                  <li style="line-height:30px"> <b>August 2024:</b> Checkout our specialized mathematical multi-modal large language model <b>MathGLM-Vision</b>!</li>
                  <li style="line-height:30px"> <b>August 2024:</b> Checkout our multi-modal scientific benchmark <b>VisScience</b>!</li>
		  <li style="line-height:30px"> <b>June 2024:</b> Checkout our GLM Team paper <a href="https://arxiv.org/abs/2406.12793">ChatGLM</a>!</li>
                  <li style="line-height:30px"> <b>February 2024:</b> Our <a href="https://ieeexplore.ieee.org/abstract/document/10454000">survey</a> paper about <b>Negative Sampling</b> is accepted by <b>TPAMI 2024</b>!</li>
                  <li style="line-height:30px"> <b>December 2023:</b> Our paper <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779TriSampler">TriSampler</a> is accepted by <b>AAAI 2024</b>!</li>
                  <li style="line-height:30px"> <b>September 2023:</b> Checkout our specialized mathematical model <a href="https://arxiv.org/abs/2309.03241"><b>MathGLM</b></a>!</li>
		  <li style="line-height:30px"> <b>September 2023:</b> I am currently doing an internship at the <a href="https://chatglm.cn/main/alltoolsdetail">ChatGLM</a> group of <a href="https://www.zhipuai.cn/">ZhipuAI</a>!</li>
		  <li style="line-height:30px"> <b>August 2023:</b> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.html">ViLTA</a> gets accepted to <b>ICCV 2023</b>!</li>
                  <li style="line-height:30px"> <b>May 2023:</b> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">BatchSampler</a> gets accepted to <b>KDD 2023</b>!</li>
                  <li style="line-height:30px"> <b>February 2022:</b> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/9723516">RecNS</a> gets accepted to <b>TKDE 2022</b>!</li>
                  <li style="line-height:30px"> <b>January 2022:</b> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512041">STAM</a> gets accepted to <b>WWW 2022</b>!</li>
		  <li style="line-height:30px"> <b>May 2021:</b> One paper <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">MixGCF</a> gets accepted to <b>KDD 2021</b>!</li>
                  <li style="line-height:30px"> <b>May 2020:</b> One paper <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">MCNS</a> gets accepted to <b>KDD 2020</b>!</li>
                <ul>
                </div>
		    </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ui2code.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/***">
                <papertitle>UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Wenyi Hong*,
              Mingde Xu, 
			  Xinyue Fan, 
			  Weihan Wang, 
			  Jiele Cheng, 
			  Xiaotao Gu,
	          Jie Tang
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2511.08195">paper</a>,
              <a href="https://github.com/zai-org/UI2Code_N">code</a>
              <p></p>
              <p>UI2Code^N is a visual language foundation model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding, which unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing.
			  </p>
            </td>
          </tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/webvia.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2511.06251">
                <papertitle>WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation</papertitle>
              </a>
              <br>
			  Mingde Xu*,
              <strong>Zhen Yang*</strong>,
              Wenyi Hong,
              Lihang Pan,
              Xinyue Fan,
	      	  Xiaotao Gu,
	          Bin Xu,
	         Jie Tang
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2511.06251">paper</a>,
			  <a href="https://github.com/zheny2751-dotcom/WebVIA">code</a>
              <p></p>
              <p>WebVIA is the first agentic framework for interactive and verifiable UI-to-Code generation. While prior vision-language models only produce static HTML/CSS layouts, WebVIA enables executable and interactive web interfaces. The framework consists of three modules: WebVIA-Agent, WebVIA-UI2Code, and Validation Module.</p>
            </td>
          </tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mathse.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/***">
                <papertitle>MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning</papertitle>
              </a>
              <br>
			  Jinhao Chen*,
              <strong>Zhen Yang*</strong>,
              Jianxin Shi,
              Tianyu Wo,
	          Jie Tang
              <br>
              <em>AAAI 2026</em>
              <br>
              <a href="https://arxiv.org/pdf/2511.06805">paper</a>,
              <a href="https://github.com/zheny2751-dotcom/MathSE">code</a>
              <p></p>
              <p>MathSE unifies distilled supervision, an Outcome Reward Model (ORM), and reflection-driven data refresh to progressively enhance math reasoning in multimodal LLMs.</p>
            </td>
          </tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/webseer.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/***">
                <papertitle>WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection</papertitle>
              </a>
              <br>
			  Guanzhong He,
              <strong>Zhen Yang</strong>,
              Jinxin Liu, 
			  Bin Xu, 
			  Lei Hou, 
			  Juanzi Li
              <br>
              <em>ICLR 2026</em>
              <br>
              <a href="https://arxiv.org/pdf/2510.18798">paper</a>,
              <a href="https://github.com/99hgz/WebSeer">code</a>
              <p></p>
              <p>WebSeer is a reinforcement learning framework for training intelligent web-based search agents capable of deeper reasoning, longer tool-use chains, and self-reflective correction.
Unlike traditional Retrieval-Augmented Generation (RAG) systems, WebSeer integrates self-reflection into every stage of reasoning, enabling agents to backtrack, reformulate queries, and iteratively improve answers in real-world web environments.</p>
            </td>
          </tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/glm-v.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/***">
                <papertitle>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>GLM-V Team</strong>,
              <br>
              <em>Technical Report</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2507.01006">paper</a>,
              <a href="https://github.com/zai-org/GLM-V">code</a>
              <p></p>
              <p>Vision-language models (VLMs) have become a key cornerstone of intelligent systems. As real-world AI tasks grow increasingly complex, VLMs urgently need to enhance reasoning capabilities beyond basic multimodal perception — improving accuracy, comprehensiveness, and intelligence — to enable complex problem solving, long-context understanding, and multimodal agents.

Through our open-source work, we aim to explore the technological frontier together with the community while empowering more developers to create exciting and innovative applications.

This open-source repository contains our GLM-4.6V, GLM-4.5V and GLM-4.1V series models. For performance and details, see Model Overview. For known issues, see Fixed and Remaining Issues.</p>
            </td>
          </tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mathglm-vision.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/***">
                <papertitle>MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Jinhao Chen*,
              Zhengxiao Du,
              Wenmeng Yu,
	      Weihan Wang,
	      Wenyi Hong,
	      Zhihuan Jiang,
	      Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>Manuscript</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2409.13729">paper</a>,
              <a href="https://github.com/THUDM/MathGLM-Vision">code</a>
              <p></p>
              <p>We aim to construct a fine-tuning dataset MathVL, and develop a series of specialized mathematical MLLMs MathGLM-Vision with various parameter-scale backbones.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/visscience.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2409.13730">
                <papertitle>VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning</papertitle>
              </a>
              <br>
	      Zhihuan Jiang*,
              <strong>Zhen Yang*</strong>,
              Jinhao Chen,
              Zhengxiao Du,
              Weihan Wang,
              Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>Manuscript</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2409.13730">paper</a>,
              <a href="https://github.com/THUDM/VisScience">code</a>
              <p></p>
              <p>We meticulously construct a comprehensive benchmark, named VisScience, which is utilized to assess the multi-modal scientific reasoning across the three disciplines of mathematics, physics, and chemistry.</p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/survey.png" alt="streamv2v" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10454000">
                <papertitle>Does Negative Sampling Matter? A Review with Insights into its Theory and Applications</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
              Tinglin Huang,
              Yukuo Cen,
              Junshuai Song,
              Bin Xu,
	      Yuxiao Dong,
	      Jie Tang
              <br>
              <em>TPAMI</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10454000">paper</a>
              <p></p>
              <p>We explore the history of negative sampling, categorize the strategies used to select negative samples, and examine their practical applications.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/trisampler.png" alt="ovseg" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779">
                <papertitle>TriSampler: A Better Negative Sampling Principle for Dense Retrieval</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Zhou Shao,
              Yuxiao Dong,
              Jie Tang,
              <br>
              <em>AAAI</em>, 2024
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28779">paper</a>
              <p></p>
              <p>We design the quasi-triangular principle and introduce TriSampler to selectively sample more informative negatives within a prescribed constrained region.</p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mathglm.png" alt="ovseg" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.03241">
                <papertitle>GPT Can Solve Mathematical Problems Without a Calculator</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Ming Ding*,
              Qingsong Lv,
              Zhihuan Jiang,
              Zehai He,
              Yuyi Guo,
              Jinfei Bai,
              Jie Tang,
              <br>
              <em>Manuscript</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2309.03241">arxiv</a>,
              <a href="https://github.com/THUDM/MathGLM">code</a>
              <p></p>
              <p>We propose a 2 billion-parameter language model MathGLM that can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage.</p>
            </td>
          </tr>
	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/batchsampler.png" alt="supmae" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">
                <papertitle>Batchsampler: Sampling Mini-batches for Contrastive Learning in Vision, Language, and Graphs</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Tinglin Huang*,
	      Ming Ding*,
	      Yuxiao Dong,
	      Rex Ying,
	      Yukuo Cen,
	      Yangliao Geng,
	      Jie Tang
              <br>
              <em>KDD</em>, 2023
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599263">paper</a>,
              <a href="https://github.com/THUDM/BatchSampler">code</a>
              <p></p>
              <p>We present BatchSampler to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances by leveraging the proximity graph and a random walk with restart.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vilta.png" alt="declip" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.pdf">
                <papertitle>ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation</papertitle>
              </a>
              <br>
              Weihan Wang*,
              <strong>Zhen Yang*</strong>,
              Bin Xu,
              Juanzi Li,
              Yankui Sun
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.pdf">paper</a>
              <p></p>
              <p>We propose a novel method ViLTA that utilize a cross-distillation method to generate soft labels for enhancing the robustness of model.</p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stam.png" alt="ant" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>STAM: A Spatiotemporal Aggregation Method for Graph Neural Network-based Recommendation</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
              Bin Xu,
              Hongxia Yang,
	      Jie Tang
              <br>
              <em>WWW</em>, 2022
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512041">paper</a>,
              <a href="https://github.com/zyang-16/STAM">code</a>
              <p></p>
              <p>We propose a spatiotemporal aggregation method STAM to efficiently incorporate temporal information into neighbor embedding learning.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/recns.png" alt="repre" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9723516">
                <papertitle>Region or Global? A Principle for Negative Sampling in Graph-based Recommendation</papertitle>
              </a>
              <br>
              <strong>Zhen Yang</strong>,
              Ming Ding,
	      Xu Zou,
	      Jie Tang,
	      Bin Xu,
	      Chang Zhou,
	      Hongxia Yang
              <br>
              <em>TKDE</em>, 2022, <b>Long oral</b>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9723516">paper</a>,
              <a href="https://github.com/zyang-16/RecNS">code</a>
              <p></p>
              <p>We design three region principle to select negative candidate and propose RecNS method to sythesize hard negatives.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mixgcf.png" alt="crnas" width="280" height="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">
                <papertitle>MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</papertitle>
              </a>
              <br>
	      Tinglin Huang,
	      Yuxiao Dong,
	      Ming Ding,
              <strong>Zhen Yang</strong>,
	      Wenzheng Feng,
	      Xinyu Wang,
	      Jie Tang
              <br>
              <em>KDD</em>, 2021
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467408">paper</a>,
              <a href="https://github.com/huangtinglin/MixGCF">code</a>
              <p></p>
              <p>We present MixGCF that can study negative sampling by leveraging both the user-item graph structure and GNNs’ aggregation process to design the hop mixing technique to synthesize hard negatives.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mcns.png" alt="oqa" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">
                <papertitle>Understanding Negative Sampling in Graph Representation Learning</papertitle>
              </a>
              <br>
              <strong>Zhen Yang*</strong>,
              Ming Ding*,
              Chang Zhou,
              Hongxia Yang,
              Jingren Zhou,
              Jie Tang,
              <br>
              <em>KDD</em>, 2020
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403218">paper</a>,
              <a href="https://github.com/THUDM/MCNS">code</a>
              <p></p>
              <p>We develop a theory and quantify that a nice negative sampling distribution is \( p_n(u|v) \propto p_d(u|v)^\alpha \), \( 0 < \alpha < 1 \). Additionly, we propose Markov chain Monte Carlo Negative Sampling (MCNS), an effective and scalable negative sampling strategy for various tasks in graph representation learning.</p>
            </td>
          </tr>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
                <p>
                  <li style="line-height:30px"> Reviewer of Journals: TKDE</li>
                  <li style="line-height:30px"> Reviewer of Conferences: ICLR 2024/2025/2026, ICCV 2025, AAAI 2025/2026, KDD 2021/2022, WWW 2023/2024/2025</li>
                </p>
            </td>
          </tr>
        </tbody></table>
	      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards and Funds</heading>
                <ul>
				  <li style="line-height:30px"> Young Scientists Fund of the National Natural Science Foundation of China (国家自然科学基金青年基金项目C类), 2025.</li>
				  <li style="line-height:30px"> General Fund of China Postdoctoral Science Foundation (中国博士后科学基金面上资助), 2025.</li>
				  <li style="line-height:30px"> China National Postdoctoral Program for Innovative Talents (2025年度博士后创新人才支持计划), 2025.</li>
				  <li style="line-height:30px"> Shuimu Tsinghua Scholar for Postdoc Researcher (清华大学水木学者), 2025.</li>
				  <li style="line-height:30px"> Beijing Outstanding Graduate, 2024.</li>
                  <li style="line-height:30px"> Huawei Scholarship, 2023.</li>
                  <li style="line-height:30px"> Excellent Graduate of Beijing, 2019.</li>
                  <li style="line-height:30px"> Outstanding Academic Paper Award, Tsinghua University, 2019.</li>
		  <li style="line-height:30px"> 129 Scholarship of Tsinghua University, 2018.</li>	
		  <li style="line-height:30px"> National Scholarship by Ministry of Education of China, 2018.</li>
                  <li style="line-height:30px"> National Scholarship by Ministry of Education of China, 2014.</li>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <a href="https://clustrmaps.com/site/1bhpp" title="Visit tracker">
                <img src="//www.clustrmaps.com/map_v2.png?d=6oa3ivKJIw5Vmqg_fFtgZxTmVsyrTJMJ_XKxZlDEsRI&cl=ffffff">
              </a>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
